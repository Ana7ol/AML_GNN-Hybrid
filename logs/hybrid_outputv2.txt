Using device: cpu

==================================================
STAGE 1: SELF-SUPERVISED LEARNING (SSL) PRE-TRAINING
==================================================

Starting preprocessing...
Dropping 'Is Laundering' column for SSL feature preparation.
Processing Timestamp...
Processing Account Numbers...
Processing Amounts...
Warning: Amount/Currency pairs are not always identical. Dropping 'Amount Paid' & 'Payment Currency'.
Processing Categorical String Features (Currency, Payment Format)...

Scaling all numerical features...
Columns to be scaled: ['From Bank', 'To Bank', 'Hour_sin', 'Hour_cos', 'Minute_sin', 'Minute_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Month_sin', 'Month_cos', 'Account_Num', 'Account.1_Num', 'Amount_Log', 'Currency_Australian Dollar', 'Currency_Bitcoin', 'Currency_Brazil Real', 'Currency_Canadian Dollar', 'Currency_Euro', 'Currency_Mexican Peso', 'Currency_Ruble', 'Currency_Rupee', 'Currency_Saudi Riyal', 'Currency_Shekel', 'Currency_Swiss Franc', 'Currency_UK Pound', 'Currency_US Dollar', 'Currency_Yen', 'Currency_Yuan', 'Format_ACH', 'Format_Bitcoin', 'Format_Cash', 'Format_Cheque', 'Format_Credit Card', 'Format_Reinvestment', 'Format_Wire']

Preprocessing finished. Final feature shape: (6924049, 35)
Creating sequences with length 10 and step 5...
Finished creating sequences. Final shape: (1503989, 10, 35)
Final labels shape: (1503989,)

SUCCESS: Sequence creation complete. N_FEATURES_PROC set to: 35
Training DataLoader created. Data shape per item: torch.Size([10, 35])

Initializing SSL models on cpu...
TransactionFeatureCNN: InputFeat=35, OutputEmb=128
TransactionSequenceEncoder_CNNthenGRU: FinalEmb=128
Projection Head: Input=128, Hidden=128, Output=32
Loss Criterion: NTXentLoss, Temperature: 0.1

--- Starting Unsupervised Training ---
Epoch [1/10], Batch [2000/11749], Loss: 0.5121, Avg Epoch Loss: 1.2927
Epoch [1/10], Batch [4000/11749], Loss: 0.3842, Avg Epoch Loss: 0.8823
Epoch [1/10], Batch [6000/11749], Loss: 0.3187, Avg Epoch Loss: 0.7020
Epoch [1/10], Batch [8000/11749], Loss: 0.2532, Avg Epoch Loss: 0.5967
Epoch [1/10], Batch [10000/11749], Loss: 0.2229, Avg Epoch Loss: 0.5251
Epoch [1/10], Batch [11749/11749], Loss: 0.2063, Avg Epoch Loss: 0.4791

--- Epoch [1/10] Summary ---
  Average Epoch Loss: 0.4791
  Epoch Duration: 4277.18s

Epoch [2/10], Batch [2000/11749], Loss: 0.1888, Avg Epoch Loss: 0.1952
Epoch [2/10], Batch [4000/11749], Loss: 0.1712, Avg Epoch Loss: 0.1877
Epoch [2/10], Batch [6000/11749], Loss: 0.1377, Avg Epoch Loss: 0.1812
Epoch [2/10], Batch [8000/11749], Loss: 0.1464, Avg Epoch Loss: 0.1756
Epoch [2/10], Batch [10000/11749], Loss: 0.1700, Avg Epoch Loss: 0.1709
Epoch [2/10], Batch [11749/11749], Loss: 0.1220, Avg Epoch Loss: 0.1670

--- Epoch [2/10] Summary ---
  Average Epoch Loss: 0.1670
  Epoch Duration: 4349.41s

Epoch [3/10], Batch [2000/11749], Loss: 0.1355, Avg Epoch Loss: 0.1424
Epoch [3/10], Batch [4000/11749], Loss: 0.1105, Avg Epoch Loss: 0.1399
Epoch [3/10], Batch [6000/11749], Loss: 0.0978, Avg Epoch Loss: 0.1378
Epoch [3/10], Batch [8000/11749], Loss: 0.1517, Avg Epoch Loss: 0.1363
Epoch [3/10], Batch [10000/11749], Loss: 0.1309, Avg Epoch Loss: 0.1346
Epoch [3/10], Batch [11749/11749], Loss: 0.1338, Avg Epoch Loss: 0.1333

--- Epoch [3/10] Summary ---
  Average Epoch Loss: 0.1333
  Epoch Duration: 4546.63s

Epoch [4/10], Batch [2000/11749], Loss: 0.0994, Avg Epoch Loss: 0.1231
Epoch [4/10], Batch [4000/11749], Loss: 0.1037, Avg Epoch Loss: 0.1223
Epoch [4/10], Batch [6000/11749], Loss: 0.1359, Avg Epoch Loss: 0.1218
Epoch [4/10], Batch [8000/11749], Loss: 0.0847, Avg Epoch Loss: 0.1212
Epoch [4/10], Batch [10000/11749], Loss: 0.1028, Avg Epoch Loss: 0.1205
Epoch [4/10], Batch [11749/11749], Loss: 0.1104, Avg Epoch Loss: 0.1200

--- Epoch [4/10] Summary ---
  Average Epoch Loss: 0.1200
  Epoch Duration: 4640.26s

